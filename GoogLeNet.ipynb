{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvlMu1Qieb28U+Arw6pLy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ro-okie/What_is-GoogLeNet/blob/main/GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing relevant libraries\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(17)"
      ],
      "metadata": {
        "id": "TyQ1qh8_2iBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([ \n",
        "    transforms.Resize((224, 224)), \n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "orfaWFtD2mPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset here\n",
        "# split train,val,test into [80000,10000,10000]\n",
        "# define train_loader. val_loader, test_loader"
      ],
      "metadata": {
        "id": "tkYh1d-Q2mom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        \n",
        "        # 1x1 conv \n",
        "        self.one_by_one = ConvBlock(in_channels, num_1x1, kernel_size=1)\n",
        "        \n",
        "        # 1x1 conv followed by 3x3 conv\n",
        "        self.tree_by_three_red = ConvBlock(in_channels, num_3x3_red, kernel_size=1)  \n",
        "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
        "        \n",
        "        \n",
        "        # 1x1 conv followed by 5x5 conv\n",
        "        self.five_by_five_red = ConvBlock(in_channels, num_5x5_red, kernel_size=1)\n",
        "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
        "        \n",
        "        \n",
        "        # 3x3 maxpool followed by 1x1 \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.pool_proj = ConvBlock(in_channels, num_pool_proj, kernel_size=1)\n",
        "         \n",
        "    def forward(self, x):\n",
        "        # 1x1 conv \n",
        "        x1 = self.one_by_one(x)\n",
        "        \n",
        "        \n",
        "        # 1x1 conv followed by 3x3 conv\n",
        "        x2 = self.tree_by_three_red(x)\n",
        "        x2 = self.tree_by_three(x2)\n",
        "        \n",
        "        \n",
        "        # 1x1 conv followed by 5x5 conv\n",
        "        x3 = self.five_by_five_red(x)\n",
        "        x3 = self.five_by_five(x3)\n",
        "        \n",
        "        \n",
        "        # 3x3 maxpool followed by 1x1 \n",
        "        x4 = self.maxpool(x)\n",
        "        x4 = self.pool_proj(x4)\n",
        "        \n",
        "        #concatenating output of all layers\n",
        "        x = torch.cat([x1, x2, x3, x4], 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e0aqRhlAnukL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=3, use_auxiliary=True, num_classes=1000):\n",
        "        super(Inception, self).__init__()\n",
        "        \n",
        "        # network's starts with :- (7x7 conv+2s, 3x3 maxpool+2s, LocalRespNorm, 1x1Conv, 3x3 conv, localrespnorm, 3x3 maxpool+2s)\n",
        "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.linear = nn.Linear(1024, num_classes)\n",
        "        \n",
        "        self.use_auxiliary = use_auxiliary\n",
        "        if use_auxiliary:\n",
        "            self.auxiliary4a = Auxiliary(512, num_classes)\n",
        "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
        "        \n",
        "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
        "        \n",
        "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
        "        \n",
        "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = None\n",
        "        z = None\n",
        "        \n",
        "        # x -> 224,224,3\n",
        "        x = self.conv1(x)\n",
        "        # 112,112,64\n",
        "        x = self.maxpool(x)\n",
        "        # 56,56,64\n",
        "        x = self.conv2(x)\n",
        "        # 56,56,192\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        \n",
        "        # inception stack start\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        # inception stack end\n",
        "        \n",
        "        # stack followed by a maxpool\n",
        "        x = self.maxpool(x)\n",
        "        # 28,28,256\n",
        "        \n",
        "        # inception stack start\n",
        "        x = self.inception4a(x)\n",
        "        if self.training and self.use_auxiliary:\n",
        "            y = self.auxiliary4a(x)\n",
        "        \n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        if self.training and self.use_auxiliary:\n",
        "            z = self.auxiliary4d(x)\n",
        "        \n",
        "        x = self.inception4e(x)\n",
        "        # inception stack end\n",
        "        \n",
        "        \n",
        "        # stack followed by maxpool\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        # inception stack\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "   \n",
        "        # inception stack end\n",
        "        \n",
        "        # stack followed by avgpool\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        \n",
        "        # flattening output\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.dropout(x)\n",
        "        # last fc\n",
        "        x = self.linear(x)\n",
        "        \n",
        "        return x, y, z"
      ],
      "metadata": {
        "id": "4Do25KBw2FX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))"
      ],
      "metadata": {
        "id": "coxm6xn92SJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Auxiliary(nn.Module):\n",
        "    # in_channels will change based on InceptionBlocks stacks\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(Auxiliary, self).__init__()\n",
        "        \n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
        "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
        "        \n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(2048, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = self.conv1x1(x)\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UIAHZCB72V7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Inception()\n",
        "#define the device to use\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "next(model.parameters()).is_cuda"
      ],
      "metadata": {
        "id": "_611B2JSALVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define everything we need for training\n",
        "epochs = 3 #50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "9SO9-LPnAPzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=50, use_auxiliary=True):\n",
        "    \n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: # Iterate over data\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
        "                    \n",
        "                    if phase == 'train': # Backward + optimize only if in training phase\n",
        "                        if use_auxiliary:\n",
        "                            outputs, aux1, aux2 = model(inputs)\n",
        "                            loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
        "                        else:\n",
        "                            outputs, _, _ = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                            \n",
        "\n",
        "                        _ , preds = torch.max(outputs, 1)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    if phase == 'val':\n",
        "                        outputs, _, _ = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            \n",
        "            if phase == 'val': # Adjust learning rate based on val loss\n",
        "                lr_scheduler.step(epoch_loss)\n",
        "                \n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "U7bfB-0k_n_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, _ = train_model(model, {\"train\": train_loader, \"val\": val_loader}, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "dRtkDn6jAvtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}